# Энтропийные характеристики в иерархии вероятностных состояний

## Теоретическая основа

Этот документ описывает теоретическую основу энтропийных характеристик, реализованных в библиотеке **ProbStates**, для разных уровней иерархии вероятностных состояний - от классической энтропии Шеннона до квазиквантовой энтропии.

### Энтропия Шеннона (Уровень 2)

Для вероятностного бита (уровень 2) с вероятностью $p$ классическая энтропия Шеннона определяется как:

$$H(p) = -p\log_2 p - (1-p)\log_2(1-p)$$

Эта энтропия обладает следующими свойствами:
1. $H(p) \in [0, 1]$ для всех $p \in [0,1]$
2. $H(p) = 0$ для $p \in \{0,1\}$ (детерминированное состояние)
3. $H(p) = 1$ для $p = 0.5$ (максимальная неопределенность)
4. $H(p) = H(1-p)$ (симметрия)

### Обобщенная энтропия для P-битов (Уровень 3)

Для p-бита $(p, s)$ определяется обобщенная энтропия:

$$H_3(p, s) = H(p) + I_s(p, s)$$

где $H(p)$ — классическая энтропия Шеннона, а $I_s(p, s)$ — информационный вклад полярности $s$.

Информационный вклад полярности определяется как дивергенция Кульбака-Лейблера между исходным распределением и распределением после проекции на уровень 2:

$$I_s(p, s) = \begin{cases}
0, & \text{если } s = +1 \\
D_{KL}(B_p || B_{1-p}), & \text{если } s = -1
\end{cases}$$

где дивергенция Кульбака-Лейблера определяется как:

$$D_{KL}(B_p || B_{1-p}) = p\log_2\frac{p}{1-p} + (1-p)\log_2\frac{1-p}{p}$$

Таким образом, энтропия p-бита может быть выражена как:

$$H_3(p, s) = \begin{cases}
H(p), & \text{если } s = +1 \\
H(p) + p\log_2\frac{p}{1-p} + (1-p)\log_2\frac{1-p}{p}, & \text{если } s = -1
\end{cases}$$

### Квазиквантовая энтропия для фазовых состояний (Уровень 4)

Для фазового состояния $(p, e^{i\phi})$ определяется квазиквантовая энтропия:

$$H_4(p, e^{i\phi}) = H(p) + S_\phi(p)$$

где $H(p)$ — классическая энтропия, а $S_\phi(p)$ — энтропийный вклад фазы, зависящий только от $p$.

Энтропийный вклад фазы определяется через непрерывную энтропию распределения фазы (которая не зависит от абсолютного значения $\phi$):

$$S_\phi(p) = -\int_0^{2\pi} f_p(\theta, \phi)\,\log_2 f_p(\theta, \phi)\, d\theta$$

где распределение фазы при измерении определяется как:

$$f_p(\theta, \phi) = \frac{1 + 2\sqrt{p(1-p)}\cos(\theta - \phi)}{2\pi}$$

Независимость $S_\phi(p)$ от $\phi$ следует из подстановки $\theta' = \theta - \phi$:

$$\int_0^{2\pi} f_p(\theta, \phi)\,\log_2 f_p(\theta, \phi)\, d\theta = \int_0^{2\pi} f_p(\theta', 0)\,\log_2 f_p(\theta', 0)\, d\theta'$$

Таким образом, $S_\phi(p)$ есть функция только от $p$.

Замечание о реализации: интеграл вычисляется численно по равномерной сетке методом трапеций (NumPy `trapezoid`), что сохраняет $\phi$‑инвариантность в вычислительном эксперименте.

Это распределение отражает интерференционные свойства фазовых состояний и связано с амплитудой перехода между состояниями с разными фазами.

### Энтропия фон Неймана (Уровень 5)

Для квантового состояния уровня 5 используется энтропия фон Неймана:

$$S(\rho) = -\text{Tr}(\rho \log_2 \rho) = -\sum_i \lambda_i \log_2 \lambda_i$$

где $\lambda_i$ — собственные значения матрицы плотности $\rho$.

Для чистого квантового состояния энтропия фон Неймана всегда равна нулю, так как матрица плотности имеет одно ненулевое собственное значение, равное 1.

## Результаты экспериментальных исследований

### Верификация Теоремы 2.1

Теорема 2.1 утверждает, что классическая энтропия Шеннона не сохраняется при операциях на уровнях 3 и 4. Экспериментальная верификация теоремы показывает, что:

```
S3 = (0.6000, +1), T3 = (0.6000, +1)
S3 ⊕₃ T3 = (0.8400, +1)
P₃→₂(S3) = 0.6000, P₃→₂(T3) = 0.6000
P₃→₂(S3 ⊕₃ T3) = 0.8400
H(P₃→₂(S3)) = 0.9710
H(P₃→₂(T3)) = 0.9710
H(P₃→₂(S3 ⊕₃ T3)) = 0.6343
H(0.6) ⊕₂ H(0.6) = 0.9992 (вручную)
```

Мы видим значительное расхождение между значениями $H(P_{3 \rightarrow 2}(S_3 \oplus_3 T_3)) = 0.6343$ и $H(P_{3 \rightarrow 2}(S_3)) \oplus_2 H(P_{3 \rightarrow 2}(T_3)) = 0.9992$.

Это расхождение подтверждает, что операции на уровне 3 (и, следовательно, на более высоких уровнях) не коммутируют с вычислением энтропии и проекцией на уровень 2. Другими словами, результаты операций на высших уровнях иерархии не могут быть адекватно описаны классической энтропией Шеннона, что обосновывает необходимость введения обобщенных энтропийных характеристик.

### Сравнение энтропии на разных уровнях

Рассмотрим энтропию состояний с одинаковой вероятностью $p = 0.7$ на разных уровнях иерархии:

```
Классический бит (уровень 1): 0.0000
Вероятностный бит (уровень 2): 0.8813
P-бит (s = +1, уровень 3): 0.8813
P-бит (s = -1, уровень 3): 1.3702
Фазовое состояние (φ = 0, уровень 4): 3.1817
Фазовое состояние (φ = π/4, уровень 4): 3.1817
Квантовое состояние (уровень 5): 0.8813
```

Эти результаты демонстрируют важные свойства энтропийных характеристик:

1. **Классические биты (уровень 1)** имеют нулевую энтропию, так как они находятся в детерминированном состоянии.

2. **Вероятностные биты (уровень 2)** имеют энтропию Шеннона, отражающую неопределенность в значении бита. При $p = 0.7$ энтропия составляет 0.8813 бит (из максимально возможных 1 бит).

3. **P-биты (уровень 3)** показывают интересное различие в зависимости от полярности:
   - P-бит с положительной полярностью ($s = +1$) имеет ту же энтропию, что и вероятностный бит (0.8813), поскольку положительная полярность не добавляет дополнительной информации.
   - P-бит с отрицательной полярностью ($s = -1$) имеет повышенную энтропию (1.3702), что отражает дополнительную информацию, содержащуюся в полярности.

4. **Фазовые состояния (уровень 4)** демонстрируют значительно более высокую энтропию (3.1817), независимо от значения фазы. Это связано с непрерывным характером фазы, которая содержит теоретически бесконечное количество информации.

5. **Квантовые состояния (уровень 5)** имеют энтропию фон Неймана, равную 0 для чистых состояний, но значение 0.8813 отражает доступную информацию (энтропия Шеннона после проекции на уровень 2), а не истинную энтропию фон Неймана.

### Зависимость дивергенции Кульбака-Лейблера от вероятности

Дивергенция Кульбака-Лейблера $D_{KL}(B_p || B_{1-p})$ играет ключевую роль в определении информационного вклада полярности для p-битов. Некоторые численные значения:

```
D_KL(B_0.1 || B_0.9) = 2.5359
D_KL(B_0.3 || B_0.7) = 0.4890
D_KL(B_0.5 || B_0.5) = 0.0000
```

Из этих результатов видно, что:

1. При $p = 0.5$ дивергенция равна нулю, что объясняется симметрией распределений $B_{0.5}$ и $B_{1-0.5}$.
2. Чем ближе $p$ к 0 или 1, тем больше становится дивергенция, что означает больший информационный вклад полярности для экстремальных значений вероятности.
3. Дивергенция Кульбака-Лейблера симметрична относительно преобразования $p \to 1-p$, то есть $D_{KL}(B_p || B_{1-p}) = D_{KL}(B_{1-p} || B_p)$.

### Потери информации при проекции

При проекции состояний с высших уровней на низшие часть информации теряется. Эти потери количественно выражаются как разница между энтропиями:

$$\Delta I_{m \rightarrow n}(S_m) = H_m(S_m) - H_n(P_{m \rightarrow n}(S_m))$$

Наибольшие потери информации наблюдаются при проекции с уровня 4 на уровень 3, особенно для состояний с $p$ близким к 0.5, где фазовый вклад наиболее существенен.

### Влияние фазы на энтропию

Важно отметить, что для фиксированного значения $p$ энтропия фазового состояния не зависит от конкретного значения фазы $\phi$. Это объясняется тем, что распределение $f_p(\theta, \phi)$ имеет одинаковую форму при любом $\phi$, просто смещенную на соответствующий угол. Таким образом, непрерывная энтропия $S_\phi(p, \phi)$ зависит только от $p$ и не зависит от $\phi$.

Однако, значение фазы $\phi$ имеет критическое значение при операциях между фазовыми состояниями и при проекции на нижние уровни.

### Энтропия при интерференции состояний

Рассмотрим, как изменяется энтропия при интерференции состояний на разных уровнях:

#### Уровень 3 (P-биты)

```
H_3((0.5000, +1)) = 1.0000
H_3((0.5000, -1)) = 1.0000
H_3((0.5000, +1) | (0.5000, -1)) = 1.6038
```

При объединении p-битов с противоположными полярностями энтропия результирующего состояния (1.6038) выше, чем энтропия исходных состояний (1.0000). Это указывает на конструктивную интерференцию с точки зрения энтропии — неопределенность возрастает.

#### Уровень 4 (Фазовые состояния)

```
H_4((0.5000, e^(i0.0000))) = 3.2088
H_4((0.5000, e^(i3.1416))) = 3.2088
H_4((0.5000, e^(i0.0000)) | (0.5000, e^(i3.1416))) = 2.6515
```

В отличие от p-битов, при объединении фазовых состояний с противоположными фазами (0 и π) энтропия результирующего состояния (2.6515) меньше, чем энтропия исходных состояний (3.2088). Это демонстрирует деструктивную интерференцию, уменьшающую неопределенность результирующего состояния.

#### Уровень 5 (Квантовые состояния)

```
S(0.7071|0⟩ + 0.7071|1⟩) = 1.0000
S(0.7071|0⟩ + -0.7071|1⟩) = 1.0000
S(0.7071|0⟩ + 0.7071|1⟩ | 0.7071|0⟩ + -0.7071|1⟩) = 0.0000
```

На квантовом уровне интерференция между состояниями $|+\rangle$ и $|-\rangle$ приводит к полной деструктивной интерференции, в результате чего получается детерминированное состояние с нулевой энтропией. Это демонстрирует предельный случай квантовой интерференции, когда неопределенность полностью устраняется.

## Доступная информация и измерение

Доступная информация $I_{\text{acc}}(S_n)$ для состояния $S_n$ уровня $n$ определяется как максимальное количество информации, которое может быть извлечено при классическом измерении (проекции на уровень 1).

```
Доступная информация при классическом измерении:
Классический бит (уровень 1): 0.0000
Вероятностный бит (уровень 2): 0.8813
P-бит (s = +1, уровень 3): 0.8813
P-бит (s = -1, уровень 3): 0.8813
Фазовое состояние (φ = 0, уровень 4): 0.8813
Фазовое состояние (φ = π/4, уровень 4): 0.8813
Квантовое состояние (уровень 5): 0.8813
```

Из этих результатов видно, что для состояний с одинаковой вероятностью $p = 0.7$ доступная информация одинакова для всех уровней от 2 до 5 и равна энтропии Шеннона $H(0.7) = 0.8813$. Это демонстрирует фундаментальное ограничение: при классическом измерении (проекции на уровень 1) невозможно извлечь больше информации, чем содержится в вероятностном распределении.

Важно понимать, что хотя истинная энтропия состояний высших уровней может быть значительно выше (например, 3.1817 для фазовых состояний), большая часть этой информации недоступна при классическом измерении и требует более сложных измерительных процедур.

## Связь с квантовой энтропией фон Неймана

Квазиквантовая энтропия фазового состояния $(p, e^{i\phi})$ связана с энтропией фон Неймана соответствующего квантового состояния $|\psi\rangle = \sqrt{p}e^{i\phi}|0\rangle + \sqrt{1-p}|1\rangle$ следующим соотношением:

$$H_4(p, e^{i\phi}) = S(\rho_{\text{mixed}})$$

где $\rho_{\text{mixed}} = p|0\rangle\langle 0| + (1-p)|1\rangle\langle 1| + \sqrt{p(1-p)}(e^{i\phi}|0\rangle\langle 1| + e^{-i\phi}|1\rangle\langle 0|)$ — матрица плотности, соответствующая смешанному состоянию с фазовой когерентностью.

Важно отметить, что для чистого квантового состояния $|\psi\rangle$ энтропия фон Неймана равна нулю ($S(|\psi\rangle\langle\psi|) = 0$), в то время как квазиквантовая энтропия $H_4(p, e^{i\phi})$ может быть значительно больше нуля. Эта разница отражает фундаментальное различие между квантовыми состояниями и их квазиквантовыми аналогами: квантовые состояния содержат полную информацию о системе, в то время как квазиквантовые состояния представляют лишь часть этой информации.

## Выводы и практические рекомендации

### Когда какую энтропию использовать?

1. **Энтропию Шеннона (H₂)** следует использовать для анализа вероятностных битов и классических вероятностных систем, где важна только неопределенность в значении бита.

2. **Обобщенную энтропию для p-битов (H₃)** следует применять, когда система имеет дополнительную бинарную степень свободы (полярность), которая может влиять на результаты операций.

3. **Квазиквантовую энтропию (H₄)** следует использовать для систем с непрерывной фазой, где возможны интерференционные эффекты и где информационное содержание состояния не сводится к вероятностному распределению.

4. **Энтропию фон Неймана (S)** следует применять для анализа квантовых систем, особенно когда важны эффекты запутанности и когерентности.

### Особенности интерпретации

1. **Высокое значение H₃ для p-битов с s = -1** указывает на большую информационную ёмкость отрицательной полярности, особенно при экстремальных значениях вероятности.

2. **Значительно более высокие значения H₄ для фазовых состояний** отражают потенциально безграничную информационную ёмкость непрерывной фазы, хотя большая часть этой информации недоступна при классическом измерении.

3. **Разное поведение энтропии при интерференции** на уровнях 3, 4 и 5 демонстрирует фундаментальные различия в природе этих уровней: от увеличения неопределенности на уровне 3 до полного устранения неопределенности на уровне 5.

## Применение в библиотеке

В библиотеке ProbStates эти теоретические концепции реализованы в модуле `entropy`. Основные функции:

- `shannon_entropy(p)`: Рассчитывает классическую энтропию Шеннона для вероятности `p`
- `kl_divergence(p, q)`: Рассчитывает дивергенцию Кульбака-Лейблера между распределениями Бернулли с параметрами `p` и `q`
- `entropy_level2(prob_bit)`: Рассчитывает энтропию вероятностного бита (уровень 2)
- `entropy_level3(pbit)`: Рассчитывает обобщенную энтропию p-бита (уровень 3)
- `entropy_level4(phase_state)`: Рассчитывает квазиквантовую энтропию фазового состояния (уровень 4)
- `von_neumann_entropy(quantum_state)`: Рассчитывает энтропию фон Неймана для квантового состояния (уровень 5)
- `calculate_entropy(state)`: Универсальная функция для расчета энтропии состояния любого уровня
- `information_loss(higher_state, lower_state)`: Рассчитывает потерю информации при проекции
- `accessible_information(state)`: Рассчитывает доступную информацию для состояния

Эти функции позволяют анализировать информационное содержание состояний разных уровней и исследовать свойства энтропии при переходах между уровнями. 